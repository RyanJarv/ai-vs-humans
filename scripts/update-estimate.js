#!/usr/bin/env node
/*
  Rough sampling-based estimate for AI vs Human-authored code lines on GitHub.

  Approach (heuristic, low-rate, transparent):
  - For a small set of languages, sample files that explicitly contain phrases
    like "generated by chatgpt/copilot/claude" and count their lines
    (treated as AI-likely lines).
  - Sample a different set of typical code files (excluding AI markers) and
    count their lines (treated as Human-likely lines).
  - Update data/estimate.json with the summed sample counts and caveats.

  Caveats: This underestimates AI-written code (most AI tools don’t insert
  markers). It’s a directional signal only, not a census.
*/

const fs = require('fs');
const path = require('path');

const TOKEN = process.env.GITHUB_TOKEN || process.env.GH_TOKEN;
if (!TOKEN) {
  console.error('GITHUB_TOKEN not found. Exiting.');
  process.exit(0); // exit gracefully so Pages builds still succeed
}

const AI_MARKERS = [
  'generated by chatgpt',
  'generated with chatgpt',
  'generated by copilot',
  'generated by claude',
  'generated by openai',
  'ai-generated',
  'created with ai'
];

// Languages to sample (balance popularity, ecosystem diversity, and cost)
const LANGS = ['JavaScript', 'TypeScript', 'Python', 'Go'];

// Sampling budget (kept small to respect rate limits)
const PER_LANG_AI_MAX = 20; // max AI-marked files per language
const PER_LANG_HUMAN_MAX = 40; // max human-likely files per language

const ROOT = process.cwd();
const DATA_FILE = path.join(ROOT, 'data', 'estimate.json');

function sleep(ms) { return new Promise(r => setTimeout(r, ms)); }

async function gh(url, opts = {}) {
  const headers = {
    'Authorization': `Bearer ${TOKEN}`,
    'User-Agent': 'ai-vs-humans-estimator',
    'Accept': 'application/vnd.github+json',
    ...opts.headers,
  };
  const res = await fetch(url, { ...opts, headers });
  if (res.status === 403) {
    const rl = res.headers.get('x-ratelimit-remaining');
    const rreset = res.headers.get('x-ratelimit-reset');
    if (rl === '0' && rreset) {
      const wait = Math.max(0, Number(rreset) * 1000 - Date.now()) + 2000;
      console.warn(`Rate limit hit, waiting ${Math.round(wait/1000)}s...`);
      await sleep(Math.min(wait, 30_000)); // cap wait to 30s
      return gh(url, opts);
    }
  }
  if (!res.ok) {
    const body = await res.text();
    throw new Error(`GitHub API ${res.status}: ${body.slice(0,200)}`);
  }
  return res;
}

async function searchCode(query, perPage = 50) {
  const url = new URL('https://api.github.com/search/code');
  url.searchParams.set('q', query);
  url.searchParams.set('per_page', String(perPage));
  // We keep pages=1 to respect low rate usage
  const res = await gh(url.toString(), {
    headers: { 'Accept': 'application/vnd.github.text-match+json' }
  });
  const json = await res.json();
  return Array.isArray(json.items) ? json.items : [];
}

async function getFileLines(owner, repo, path) {
  const url = `https://api.github.com/repos/${owner}/${repo}/contents/${encodeURIComponent(path)}`;
  const res = await gh(url);
  const json = await res.json();
  if (!json || !json.content) return 0;
  const buff = Buffer.from(json.content, 'base64');
  const text = buff.toString('utf8');
  return text.split('\n').length;
}

function uniqueFileKey(item) {
  // item: { repository: { full_name }, path, sha }
  const full = item.repository && item.repository.full_name ? item.repository.full_name : 'unknown/unknown';
  return `${full}#${item.path}#${item.sha || ''}`;
}

async function sampleAiFilesForLang(lang) {
  const picked = new Map();
  for (const marker of AI_MARKERS) {
    if (picked.size >= PER_LANG_AI_MAX) break;
    const q = `${marker} in:file language:${lang}`;
    try {
      const items = await searchCode(q, 50);
      for (const it of items) {
        if (picked.size >= PER_LANG_AI_MAX) break;
        const k = uniqueFileKey(it);
        if (!picked.has(k)) picked.set(k, it);
      }
    } catch (e) {
      console.warn('search error (AI):', e.message);
    }
  }
  return [...picked.values()];
}

async function sampleHumanFilesForLang(lang) {
  const picked = new Map();
  // Simple language-specific queries that likely return normal code files
  const patterns = lang === 'Go' ? ['package ', 'func '] : lang === 'Python' ? ['def ', 'class '] : ['function ', 'class '];
  for (const pat of patterns) {
    if (picked.size >= PER_LANG_HUMAN_MAX) break;
    const noclaims = AI_MARKERS.map(p => `-"${p}"`).join(' ');
    const q = `${pat} in:file language:${lang} ${noclaims}`;
    try {
      const items = await searchCode(q, 50);
      for (const it of items) {
        if (picked.size >= PER_LANG_HUMAN_MAX) break;
        const k = uniqueFileKey(it);
        if (!picked.has(k)) picked.set(k, it);
      }
    } catch (e) {
      console.warn('search error (Human):', e.message);
    }
  }
  return [...picked.values()];
}

async function countLinesFromItems(items) {
  let total = 0;
  for (const it of items) {
    const repo = it.repository && it.repository.full_name ? it.repository.full_name : '';
    const [owner, name] = repo.split('/');
    if (!owner || !name) continue;
    try {
      const lines = await getFileLines(owner, name, it.path);
      total += Number.isFinite(lines) ? lines : 0;
    } catch (e) {
      // Skip files that fail to fetch or are too large
      console.warn(`skip ${repo}/${it.path}: ${e.message}`);
    }
    // small pacing to be nice to API
    await sleep(200);
  }
  return total;
}

async function main() {
  let aiLines = 0;
  let humanLines = 0;

  for (const lang of LANGS) {
    console.log(`Sampling ${lang}…`);
    const aiItems = await sampleAiFilesForLang(lang);
    const humanItems = await sampleHumanFilesForLang(lang);
    console.log(`  AI-likely files: ${aiItems.length}, Human-likely files: ${humanItems.length}`);

    aiLines += await countLinesFromItems(aiItems);
    humanLines += await countLinesFromItems(humanItems);
  }

  // If sampling yields nothing (e.g., rate limit), keep previous values.
  if (aiLines === 0 && humanLines === 0) {
    console.warn('No sample gathered; aborting update to keep existing data.');
    return;
  }

  const out = {
    updated_at: new Date().toISOString(),
    scope: 'Sampled public GitHub code via code search (JS/TS/Python/Go)',
    ai_lines: aiLines,
    human_lines: humanLines,
    notes: 'Heuristic based on explicit AI markers (e.g., "generated by ChatGPT/Copilot/Claude"). Likely underestimates AI-written code. Counts reflect sampled files only.'
  };

  fs.mkdirSync(path.dirname(DATA_FILE), { recursive: true });
  fs.writeFileSync(DATA_FILE, JSON.stringify(out, null, 2) + '\n');
  console.log('Wrote', DATA_FILE, out);
}

main().catch(err => {
  console.error('Failed to update estimate:', err);
  process.exit(0); // do not fail the workflow; keep site deployable
});
